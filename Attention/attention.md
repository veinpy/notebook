# Attention Knowledge

## Self-Attention


## dot-production

## Query, Key, Value
[Attention is all you need](https://arxiv.org/abs/1706.03762)
>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors

