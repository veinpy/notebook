### Interesting Paradox

> [Skipgram isn't Matrix Factorisation](http://building-babylon.net/2016/05/12/skipgram-isnt-matrix-factorisation/) **vs.** [Neural Word Embedding as Implicit Matrix Factorization](https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf)

> [如何看待Yoav Goldberg 怒怼来自MILA的GAN for NLG的paper?](https://www.zhihu.com/question/60902505)

> [A Response to Yann LeCun’s Response](https://medium.com/@yoav.goldberg/a-response-to-yann-lecuns-response-245125295c02)

> [CNN Is All You Need](https://arxiv.org/abs/1712.09662) **vs.** [Attention is All you need]()

> [Everything that Works Works Because it's Bayesian: Why Deep Nets Generalize?](http://www.inference.vc/everything-that-works-works-because-its-bayesian-2/) **vs.** [The Generalization Mystery: Sharp vs Flat Minima](http://www.inference.vc/sharp-vs-flat-minima-are-still-a-mystery-to-me/)