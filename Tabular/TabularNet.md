# Tabular Neural Network

## Wide&Deep 

Wide&Deep jointly trains wide linear models and deep neural networks to combine the benefits of memorization and generalization for real-world recommender systems. The model was productionized and evaluated on Google Play.

https://paperswithcode.com/method/wide-deep

## TaBERT

TaBERT is a pretrained LM that jointly learns representations for natural language sentences and (semi-)structured tables. TaBERT works well for semantic parsing and is trained on a large corpus of 26 million tables and their English contexts.

https://paperswithcode.com/method/tabert


## TabTransformer

TabTransformer is a deep tabular data modeling architecture for supervised and semi-supervised learning. It is built upon self-attention based Transformers. The model learns robust contextual embeddings to achieve higher prediction accuracy.

https://paperswithcode.com/method/tabtransformer


## SAINT

SAINT is a recent hybrid deep learning approach for tabular data. It performs attention over both rows and columns, and it includes an enhanced embedding method. It outperforms gradient boosting methods like CatBoost on a variety of benchmark tasks.

https://paperswithcode.com/method/saint


## FT-Transformer

FT-Transformer is a Transformer-based architecture for the tabular domain. The model transforms all features (categorical and numerical) to tokens and runs a stack of Transformer layers over the tokens. It outperforms other DL models on several tasks.

https://paperswithcode.com/method/ft-transformer


# Reference

paperwidecode: https://paperswithcode.com/methods/category/deep-tabular-learning


