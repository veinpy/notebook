# OpenAI Meta Learning & Self Play

## Reinforcement learning algorithms in a nutshell

	> Add a bit of randomness to your actions

	> If the result was better than expected, do more of the same in the future.

--

### Model-free RL: Two classes fo alogrithms

	> **Policy grandients**:
	>
	>	Just take the gradient
	>
	> 	Stable, easy to use
	>
	> 	Very few tricks needed
	>
	> 	On policy

	> **Q-learning bases** :
	>
	> 	Less stable, more sample efficient
	>
	> 	Won't explain how it works
	>
	> 	Off policy: can be trained on data generated by some other policy!
	>
	> 	**Q-funtion** = estimate of future returns from current state and action. Using **Bellman equation**

	```text
	**On policy**:  learn only from my own actions

	**Off policy**: learn from anyone trying to achieve any goal
	```

--

### Meta Learning
#### = learn to learn by solving many tasks

	The dream:
	> "Learn to learn"
	>
	> Train a system on many tasks
	>
	> Resulting system can solve new tasks quickly

	**Modern meta learning**

	> Reduce learning-to-learn to supervised learning
	>
	> Training case => training task
	>
	> Model consumes all training data + test case, outputs classification

--

**Some projects(Studies) Illustrated below**

### Project: Hindsight Experience Replay [Andrychowicz et al. 2017]

DDPG: Deep Deterministic Policy Gradients [2015]

	> Exploration: a key challenge
	>
	>	Random behavior must generate some reward 
	>		Otherwise learning will not occur!
	>
	> 	If reward is too sparse, learning cannot succeed in RL
	>		One approach: change the reward?
	>
	> 	If you try to achieve your goal and fail, model learns nothing
	> 
	> Higndsight Experience Replay(HER): key idea
	> 	Do not waste experience, even though it's not what you want.
	>
	>	Setup: build a system that can reach any state
	> 	Goal: reach A
	>	Any trajectory ends up in some other state B
	> 	Use this as training data to reach state B?
	
--

### Project: Sim2Real with meta learning [2017]

Key idea: randomize the simulation

	> Randomize simulation parameters (a famility of simulation sets)
	>	Gravity
	> 	Friction
	> 	Torques
	>	Width and length of differnt geometric shapes
	>	Type of contact simulation
	>	etc.

	> Train a policy that can adapt to all settings of simulation paremters
	>
	> This is a meta-learning approach

--

### Learning a hierarchy of actions [2017]

Hierarchical RL is useful - if only it worked!

	> very long horizons
	> Directed exploration
	> Credit assignment
	

Simple meta learning approach

	> Given a distribution over tasks
	> Finda hierarchy taht allows for rapid learning of new tasks from the hierarchy
	> short run of policy gradients in the inner loop
	>	backprop through the short run of the learning alogrithm

--

### Limitation of high capacity meta learning

> Reduce meta learning to conventional deep learning
>
> Good for OMNI, certain robotics tasks
>
> Key limit: training task distribution = test task distribution
>
> This assumption is alomost always false!
>	Example: can a system learn chemistry after learning reading, writing, some math, and some programming?

> For meta learning to succeed, must vastly improve generalization
> 